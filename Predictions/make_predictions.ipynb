{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from typing import Optional\n",
    "from functools import partial\n",
    "import torch\n",
    "from torch import nn\n",
    "from torch.nn import functional as F\n",
    "import torch.nn.init as init\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "\n",
    "\n",
    "\n",
    "class SimCSE(nn.Conv2d):\n",
    "  def __init__(\n",
    "    self,\n",
    "    in_channels: int,\n",
    "    out_channels: int,\n",
    "    kernel_size: int=3,\n",
    "    padding: int=0,\n",
    "    stride: int=1,\n",
    "    groups: int=1,\n",
    "    shared_weights: bool = False,\n",
    "    log_p_init: float=.7,\n",
    "    log_q_init: float=1.,\n",
    "    log_p_scale: float=5.,\n",
    "    log_q_scale: float=.3,\n",
    "    alpha: Optional[float]=10,\n",
    "    alpha_autoinit: bool=False,\n",
    "    eps: float=1e-6,\n",
    "):\n",
    "    assert groups == 1 or groups == in_channels, \" \".join([\n",
    "        \"'groups' needs to be 1 or 'in_channels' \",\n",
    "        f\"({in_channels}).\"])\n",
    "    assert out_channels % groups == 0, \" \".join([\n",
    "        \"The number of\",\n",
    "        \"output channels needs to be a multiple of the number\",\n",
    "        \"of groups.\\nHere there are\",\n",
    "        f\"{out_channels} output channels and {groups}\",\n",
    "        \"groups.\"])\n",
    "\n",
    "    self.in_channels = in_channels\n",
    "    self.out_channels = out_channels\n",
    "    self.stride = stride\n",
    "    self.groups = groups\n",
    "    self.shared_weights = shared_weights\n",
    "\n",
    "    if self.groups == 1:\n",
    "      self.shared_weights = False\n",
    "\n",
    "    super(SimCSE, self).__init__(\n",
    "        self.in_channels,\n",
    "        self.out_channels,\n",
    "        kernel_size,\n",
    "        bias=False,\n",
    "        padding=padding,\n",
    "        stride=stride,\n",
    "        groups=self.groups)\n",
    "\n",
    "    # Overwrite self.kernel_size created in the 'super' above.\n",
    "    # We want an int, assuming a square kernel, rather than a tuple.\n",
    "    self.kernel_size = kernel_size\n",
    "\n",
    "    # Scaling weights in this way generates kernels that have\n",
    "    # an l2-norm of about 1. Since they get normalized to 1 during\n",
    "    # the forward pass anyway, this prevents any numerical\n",
    "    # or gradient weirdness that might result from large amounts of\n",
    "    # rescaling.\n",
    "    self.channels_per_kernel = self.in_channels // self.groups\n",
    "    weights_per_kernel = self.channels_per_kernel * self.kernel_size ** 2\n",
    "    if self.shared_weights:\n",
    "      self.n_kernels = self.out_channels // self.groups\n",
    "    else:\n",
    "      self.n_kernels = self.out_channels\n",
    "    initialization_scale = (3 / weights_per_kernel) ** .5\n",
    "    scaled_weight = np.random.uniform(\n",
    "        low=-initialization_scale,\n",
    "        high=initialization_scale,\n",
    "        size=(\n",
    "            self.n_kernels,\n",
    "            self.channels_per_kernel,\n",
    "            self.kernel_size,\n",
    "            self.kernel_size)\n",
    "    )\n",
    "    self.weight = torch.nn.Parameter(torch.Tensor(scaled_weight))\n",
    "\n",
    "    self.log_p_scale = log_p_scale\n",
    "    self.log_q_scale = log_q_scale\n",
    "    self.p = torch.nn.Parameter(torch.full(\n",
    "        (1, self.n_kernels, 1, 1),\n",
    "        float(log_p_init * self.log_p_scale)))\n",
    "    self.q = torch.nn.Parameter(torch.full(\n",
    "        (1, 1, 1, 1), float(log_q_init * self.log_q_scale)))\n",
    "    self.eps = eps\n",
    "\n",
    "    if alpha is not None:\n",
    "      self.alpha = torch.nn.Parameter(torch.full(\n",
    "          (1, 1, 1, 1), float(alpha)))\n",
    "    else:\n",
    "      self.alpha = None\n",
    "    if alpha_autoinit and (alpha is not None):\n",
    "      self.LSUV_like_init()\n",
    "\n",
    "  def LSUV_like_init(self):\n",
    "    BS, CH = 32, int(self.weight.shape[1]*self.groups)\n",
    "    H, W = self.weight.shape[2], self.weight.shape[3]\n",
    "    device = self.weight.device\n",
    "    inp = torch.rand(BS, CH, H, W, device=device)\n",
    "    with torch.no_grad():\n",
    "        out = self.forward(inp)\n",
    "        coef = (out.std(dim=(0, 2, 3)) + self.eps).mean()\n",
    "        self.alpha.data *= 1.0 / coef.view_as(self.alpha)\n",
    "    return\n",
    "\n",
    "  def forward(self, inp: torch.Tensor) -> torch.Tensor:\n",
    "    # Scale and transform the p and q parameters\n",
    "    # to ensure that their magnitudes are appropriate\n",
    "    # and their gradients are smooth\n",
    "    # so that they will be learned well.\n",
    "    p = torch.exp(self.p / self.log_p_scale)\n",
    "    q = torch.exp(-self.q / self.log_q_scale)\n",
    "\n",
    "    # If necessary, expand out the weight and p parameters.\n",
    "    if self.shared_weights:\n",
    "        weight = torch.tile(self.weight, (self.groups, 1, 1, 1))\n",
    "        p = torch.tile(p, (1, self.groups, 1, 1))\n",
    "    else:\n",
    "        weight = self.weight\n",
    "\n",
    "    return self.scs(inp, weight, p, q)\n",
    "\n",
    "  def scs(self, inp, weight, p, q):\n",
    "    # Normalize the kernel weights.\n",
    "    weight = weight / self.weight_norm(weight)\n",
    "\n",
    "    # Normalize the inputs and\n",
    "    # Calculate the dot product of the normalized kernels and the\n",
    "    # normalized inputs.\n",
    "    cos_sim = F.conv2d(\n",
    "        inp,\n",
    "        weight,\n",
    "        stride=self.stride,\n",
    "        padding=self.padding,\n",
    "        groups=self.groups,\n",
    "    ) / self.input_norm(inp, q)\n",
    "\n",
    "    # Raise the result to the power p, keeping the sign of the original.\n",
    "    out = cos_sim.sign() * (cos_sim.abs() + self.eps) ** p\n",
    "\n",
    "    # Apply learned scale parameter\n",
    "    if self.alpha is not None:\n",
    "      out = self.alpha.view(1, -1, 1, 1) * out\n",
    "    return out\n",
    "\n",
    "  def weight_norm(self, weight):\n",
    "    # Find the l2-norm of the weights in each kernel.\n",
    "    return weight.square().sum(dim=(1, 2, 3), keepdim=True).sqrt()\n",
    "\n",
    "  def input_norm(self, inp, q):\n",
    "    # Find the l2-norm of the inputs at each position of the kernels.\n",
    "    # Sum the squared inputs over each set of kernel positions\n",
    "    # by convolving them with the mock all-ones kernel weights.\n",
    "    xnorm = F.conv2d(\n",
    "        inp.square(),\n",
    "        torch.ones((\n",
    "            self.groups,\n",
    "            self.channels_per_kernel,\n",
    "            self.kernel_size,\n",
    "            self.kernel_size)),\n",
    "        stride=self.stride,\n",
    "        padding=self.padding,\n",
    "        groups=self.groups)\n",
    "\n",
    "    # Add in the q parameter. \n",
    "    xnorm = (xnorm + self.eps).sqrt() + q\n",
    "    outputs_per_group = self.out_channels // self.groups\n",
    "    return torch.repeat_interleave(xnorm, outputs_per_group, axis=1)\n",
    "\n",
    "\n",
    "class AbsPool(nn.Module):\n",
    "  def __init__(self, pooling_module=None, *args, **kwargs):\n",
    "    super(AbsPool, self).__init__()\n",
    "    self.pooling_layer = pooling_module(*args, **kwargs)\n",
    "\n",
    "  def forward(self, x: torch.Tensor) -> torch.Tensor:\n",
    "    pos_pool = self.pooling_layer(x)\n",
    "    neg_pool = self.pooling_layer(-x)\n",
    "    abs_pool = torch.where(pos_pool >= neg_pool, pos_pool, -neg_pool)\n",
    "    return abs_pool\n",
    "\n",
    "MaxAbsPool2d = partial(AbsPool, nn.MaxPool2d)\n",
    "\n",
    "class Network(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "\n",
    "        self.scs1 = SimCSE(\n",
    "            in_channels=n_input_channels,\n",
    "            out_channels=n_units_1,\n",
    "            kernel_size=5,\n",
    "            padding=0)\n",
    "        self.pool1 = MaxAbsPool2d(kernel_size=2, stride=2, ceil_mode=True)\n",
    "\n",
    "        self.scs2 = SimCSE(\n",
    "            in_channels=n_units_1,\n",
    "            out_channels=n_units_2,\n",
    "            kernel_size=5,\n",
    "            padding=1)\n",
    "        self.pool2 = MaxAbsPool2d(kernel_size=2, stride=2, ceil_mode=True)\n",
    "\n",
    "        self.scs3 = SimCSE(\n",
    "            in_channels=n_units_2,\n",
    "            out_channels=n_units_3,\n",
    "            kernel_size=5,\n",
    "            padding=1)\n",
    "        self.pool3 = MaxAbsPool2d(kernel_size=4, stride=4, ceil_mode=True)\n",
    "        self.out = nn.Linear(in_features=3600, out_features=len(classes))\n",
    "\n",
    "    def n_params(self):\n",
    "        n = 0\n",
    "        for scs in [self.scs1, self.scs2, self.scs3]:\n",
    "            n += (\n",
    "                np.prod(scs.weight.shape) +\n",
    "                np.prod(scs.p.shape) +\n",
    "                np.prod(scs.q.shape))\n",
    "        n += np.prod(self.out.weight.shape)\n",
    "        return n\n",
    "\n",
    "    def forward(self, t):\n",
    "        t = self.scs1(t)\n",
    "        t = self.pool1(t)\n",
    "\n",
    "        t = self.scs2(t)\n",
    "        t = self.pool2(t)\n",
    "        \n",
    "        t = self.scs3(t)\n",
    "        t = self.pool3(t)\n",
    "\n",
    "        t = t.view(t.size(0), -1)\n",
    "        t = self.out(t)\n",
    "\n",
    "        return t"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Net(nn.Module):\n",
    "  def __init__(self):\n",
    "    super().__init__()\n",
    "    self.conv1 = nn.Conv2d(3, 6, 5)\n",
    "    self.pool = nn.MaxPool2d(2, 2)\n",
    "    self.conv2 = nn.Conv2d(6, 16, 5)\n",
    "    self.fc1 = nn.Linear(59536, 128)\n",
    "    self.fc2 = nn.Linear(128, 128)\n",
    "    self.fc3 = nn.Linear(128, 128)\n",
    "\n",
    "  def forward(self, x):\n",
    "    x = self.pool(F.relu(self.conv1(x)))\n",
    "    x = self.pool(F.relu(self.conv2(x)))\n",
    "    x = torch.flatten(x, 1)  # flatten all dimensions except batch\n",
    "    x = F.relu(self.fc1(x))\n",
    "    x = F.relu(self.fc2(x))\n",
    "    x = self.fc3(x)\n",
    "    return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch \n",
    "net = torch.load('/Users/andrewargeros/Documents/CDS-5950-Capstone/Models/convolution.pt',\n",
    "    map_location=torch.device('cpu'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Net(\n",
       "  (conv1): Conv2d(3, 6, kernel_size=(5, 5), stride=(1, 1))\n",
       "  (pool): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "  (conv2): Conv2d(6, 16, kernel_size=(5, 5), stride=(1, 1))\n",
       "  (fc1): Linear(in_features=59536, out_features=128, bias=True)\n",
       "  (fc2): Linear(in_features=128, out_features=128, bias=True)\n",
       "  (fc3): Linear(in_features=128, out_features=128, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "net "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "import albumentations as A\n",
    "from albumentations.pytorch import ToTensorV2\n",
    "\n",
    "test_transforms = A.Compose(\n",
    "    [\n",
    "        A.SmallestMaxSize(max_size=350),\n",
    "        A.CenterCrop(height=256, width=256),\n",
    "        A.Normalize(mean=(0.485, 0.456, 0.406), std=(0.229, 0.224, 0.225)),\n",
    "        ToTensorV2(),\n",
    "    ]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('Dark Malty Beers', 30.145248413085938),\n",
       " ('Light Beers', 21.284114837646484),\n",
       " ('Stouts', 18.034154891967773),\n",
       " ('Fruit Beer', 15.659406661987305),\n",
       " ('IPA', 8.632295608520508)]"
      ]
     },
     "execution_count": 61,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "beer = '/Users/andrewargeros/Downloads/IMG_3448.JPG'\n",
    "t = test_transforms(image=cv2.imread(beer))\n",
    "\n",
    "b = torch.unsqueeze(t['image'], 0)\n",
    "net.eval()\n",
    "out = net(b)\n",
    "prob = torch.nn.functional.softmax(out, dim=1)[0] * 100\n",
    "_, indices = torch.sort(out, descending=True)\n",
    "\n",
    "classes = ['Dark Malty Beers',\n",
    "           'Fruit Beer',\n",
    "           'IPA',\n",
    "           'Light Beers',\n",
    "           'nan',\n",
    "           'NOT APPLICABLE',\n",
    "           'Stouts']\n",
    "[(classes[idx], prob[idx].item()) for idx in indices[0][:5]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[  0.5031,   0.4296,  -0.0826,   0.9459, -12.7544,  -1.4035,  -0.1533,\n",
       "         -12.5288, -12.7742, -12.7771, -13.3568, -11.8579, -14.3833, -12.9176,\n",
       "         -12.5038, -12.2867, -12.4958, -13.1342, -13.4447, -13.3530, -13.4480,\n",
       "         -12.6563, -12.3481, -13.5340, -12.4730, -13.4127, -12.8176, -13.2121,\n",
       "         -13.2459, -13.6531, -12.9309, -12.9689, -12.7460, -14.0753, -13.5890,\n",
       "         -12.7821, -12.7713, -13.1488, -13.5154, -13.0589, -13.3421, -12.0689,\n",
       "         -13.7885, -13.1374, -13.2224, -12.5681, -12.8236, -13.3941, -12.6402,\n",
       "         -13.3742, -13.4391, -13.9239, -13.5879, -12.5649, -12.2784, -12.0253,\n",
       "         -12.2664, -12.4480, -13.2110, -11.9693, -12.2393, -13.8797, -13.1799,\n",
       "         -11.3393, -13.8272, -13.1139, -13.5190, -12.8256, -13.8177, -14.0785,\n",
       "         -12.5435, -13.6326, -12.6651, -12.1869, -13.6152, -13.3730, -13.4342,\n",
       "         -13.4093, -12.8215, -12.8179, -13.4719, -13.5709, -13.7991, -13.6349,\n",
       "         -13.2671, -12.8880, -13.2179, -13.9519, -13.9469, -13.4605, -13.1139,\n",
       "         -12.5056, -13.5483, -12.5106, -13.6265, -12.4873, -13.7234, -13.3295,\n",
       "         -14.1493, -13.1407, -14.1397, -13.8698, -13.4138, -13.0296, -13.3179,\n",
       "         -12.5227, -14.0385, -13.1252, -12.8458, -13.4687, -13.7090, -13.2834,\n",
       "         -13.3269, -13.1345, -11.6295, -12.7888, -12.8236, -13.1054, -12.4282,\n",
       "         -13.5839, -13.3185, -14.3033, -13.6815, -11.8935, -12.7536, -12.3021,\n",
       "         -12.7925, -13.3329]], grad_fn=<AddmmBackward0>)"
      ]
     },
     "execution_count": 54,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "c6e4e9f98eb68ad3b7c296f83d20e6de614cb42e90992a65aa266555a3137d0d"
  },
  "kernelspec": {
   "display_name": "Python 3.9.10 ('base')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.10"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
